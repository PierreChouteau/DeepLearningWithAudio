# Training GANSynth

This guide describes how to train a GANSynth model using your own sounds. Note that training is a very computation-intensive process, so you will want to use a powerful GPU.

----

## Using Aalto computers

If you want to train on Aalto computers, see our [Using Aalto computers](../../../using-aalto-computers.md) document.

For training on the Triton computing cluster, see also [Training GANSynth on Triton](triton/README.md).

----

## Create a dataset

Set up a [Conda environment for GANSynth](../README.md) and activate it. Enter the `gansynth/training` directory.

Prepare any amount of audio samples (the more the better) and put them in one directory. The requirements for the samples are:

- WAV format
- 16000 Hz sample rate
- 16-bit signed integer
- 4 second duration (64000 frames)
- Mono
- Named like `[instrument]_[pitch].wav`, e.g. `piano_62.wav`

You can use the `chop.py` script to automatically chop up longer files (such as music). For example, if you have music files in a folder called `mytunes`, you can run:

```
python chop.py --step 16000 mytunes mysamples
```

This will create suitable 4-second files in the `mysamples` folder. The `--step` parameter specifies how much to advance in the audio at a time — for example, the step size of `16000` frames used above will advance 1 second at a time, causing 3 seconds of overlap between successive output files. Using `64000` would produce non-overlapping files, etc.

GANSynth expects input in the TFRecord format (a generic file format for TensorFlow data), so the WAV files need to be converted. This can be done with our script `make_dataset.py`.

Run `make_dataset.py` as follows:

```
python make_dataset.py --in_dir mysamples --out_dir mydataset
```

This will look for samples in the directory `mysamples` and produce output in the directory `mydataset`.

On success, the script outputs two files:

- `data.tfrecord` — The actual training data
- `meta.json` — Metadata about the samples

## Run the training

Have a look at [Magenta's training instructions](https://github.com/tensorflow/magenta/tree/master/magenta/models/gansynth#training) to get the basic idea.

Our version of Magenta takes some additional options in `--hparams`:

- `train_meta_path` — Path to the `meta.json` file generated by `make_dataset.py`
- `train_instrument_sources` - List of [instrument sources](https://magenta.tensorflow.org/datasets/nsynth#instrument-sources) to include (default `[0]`)
- `train_min_pitch`, `train_max_pitch` - Minimum/maximum pitches to include (default `24`, `84`)

Datasets generated by `make_datasets.py` have instrument source `0`, so it's usually not necessary to specify explicitly.

Run `gansynth_train`: 

```
gansynth_train \
    --config=mel_prog_hires \
    --hparams='{"train_data_path":"mydataset/data.tfrecord", "train_meta_path":"mydataset/meta.json", "train_root_dir":"mymodel", "dataset_name":"nsynth_tfrecord"}'
```

Note that this is a single command, so make sure to include all the lines if copy-pasting!

This will load the training data from `mydataset/data.tfrecord` and metadata from `mydataset/meta.json`, and generate checkpoints in the `mymodel` directory.

## Postprocess the trained model

To generate sounds using the trained model, some postprocessing is required to remove intermediate files, fix up paths etc. Our `postprocess_model.py` script will do all of this. Run it as follows:

```
python postprocess_model.py --ckpt_dir mymodel --meta_path mydataset/meta.json
```

The script will list the required postprocessing steps. After checking that everything looks reasonable, re-run the script with the `--execute` flag to actually perform the postprocessing:

```
python postprocess_model.py --ckpt_dir mymodel --meta_path mydataset/meta.json --execute
```

Your model is now ready to be used with `gansynth_generate` or our Pd patches!
