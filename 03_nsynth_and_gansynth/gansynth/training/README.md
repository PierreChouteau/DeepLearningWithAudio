# Training GANSynth

This guide describes how to train a GANSynth model using your own sounds. Note that training is a very computation-intensive process, so you will want to use a powerful GPU.

----

## Using Aalto computers

If you want to train on Aalto computers, see our [Using Aalto computers](../../../using-aalto-computers.md) document.

For training on the Triton computing cluster, see also [Training GANSynth on Triton](triton/README.md).

----

## Set up a Conda environment

Note: Previously we suggested following the [main GANSynth guide](../README.md), but because of problems with that approach we now have separate instructions here.

Enter the GANSynth training directory:

```
cd DeepLearningWithAudio/03_nsynth_and_gansynth/gansynth/training
```

Create a Conda environment using the `gansynth-training-env.yml` file:

```
conda env create -n magenta -f gansynth-training-env.yml
```

Activate the environment:

```
conda activate magenta
```

Enter the Magenta directory: (here we assume you cloned Magenta to `DeepLearningAudio/magenta`, adjust accordingly if not)

```
cd ../../../magenta
```

Install Magenta:

```
pip install -e .
```

If you get an error related to `python-rtmidi`, open `setup.py` in a text editor and search for `rtmidi`. Comment out that line by adding a `#` to the start:

```
...
    'pygtrie >= 2.3',
    #'python-rtmidi >= 1.1, < 1.2',  # 1.2 breaks us
    'scikit-image',
...
```

Then run the install command again.

## Create a dataset

Enter the `gansynth/training` directory which contains our preparation scripts:

```
# if you're in DeepLearningWithAudio/magenta after the previous steps:
cd ..

cd 03_nsynth_and_gansynth/gansynth/training
```

### From longer recordings

You can use the `chop.py` script to automatically chop up longer files (such as music). For example, if you have music files in a folder called `mytunes`, you can run:

```
python chop.py --step 16000 mytunes mysamples
```

This will create suitable 4-second files in the `mysamples` folder. The `--step` parameter specifies how much to advance in the audio at a time — for example, the step size of `16000` frames used above will advance 1 second at a time, causing 3 seconds of overlap between successive output files. Using `64000` would produce non-overlapping files, etc.

### From separate files

If your dataset consists of separate files (e.g. single percussion hits), you can use the `pad.py` script to convert them into the proper format. For example, if your files are in a folder called `myfiles`, you can run:

```
python pad.py myfiles mysamples
```

This will create suitable 4-second files in the `mysamples` folder, padding shorter files and trimming longer ones.

### Manually

The requirements for the samples are:

- WAV format
- 16000 Hz sample rate
- 16-bit signed integer
- 4 second duration (64000 frames)
- Mono
- Named like `[instrument]_[pitch].wav`, e.g. `piano_62.wav`

## Convert to TFRecord

GANSynth expects input in the TFRecord format (a generic file format for TensorFlow data), so the WAV files need to be converted. This can be done with our script `make_dataset.py`.

Run `make_dataset.py` as follows:

```
python make_dataset.py --in_dir mysamples --out_dir mydataset
```

This will look for samples in the directory `mysamples` and produce output in the directory `mydataset`.

On success, the script outputs two files:

- `data.tfrecord` — The actual training data
- `meta.json` — Metadata about the samples

## Run the training

Have a look at [Magenta's training instructions](https://github.com/tensorflow/magenta/tree/master/magenta/models/gansynth#training) to get the basic idea.

Our version of Magenta takes some additional options in `--hparams`:

- `train_meta_path` — Path to the `meta.json` file generated by `make_dataset.py`
- `train_instrument_sources` - List of [instrument sources](https://magenta.tensorflow.org/datasets/nsynth#instrument-sources) to include (default `[0]`)
- `train_min_pitch`, `train_max_pitch` - Minimum/maximum pitches to include (default `24`, `84`)

Datasets generated by `make_datasets.py` have instrument source `0`, so it's usually not necessary to specify explicitly.

Run `gansynth_train`: 

```
gansynth_train \
    --config=mel_prog_hires \
    --hparams='{"train_data_path":"mydataset/data.tfrecord", "train_meta_path":"mydataset/meta.json", "train_root_dir":"mymodel", "dataset_name":"nsynth_tfrecord"}'
```

Note that this is a single command, so make sure to include all the lines if copy-pasting!

This will load the training data from `mydataset/data.tfrecord` and metadata from `mydataset/meta.json`, and generate checkpoints in the `mymodel` directory.

## Postprocess the trained model

To generate sounds using the trained model, some postprocessing is required to remove intermediate files, fix up paths etc. Our `postprocess_model.py` script will do all of this. Run it as follows:

```
python postprocess_model.py --ckpt_dir mymodel --meta_path mydataset/meta.json
```

The script will list the required postprocessing steps. After checking that everything looks reasonable, re-run the script with the `--execute` flag to actually perform the postprocessing:

```
python postprocess_model.py --ckpt_dir mymodel --meta_path mydataset/meta.json --execute
```

Your model is now ready to be used with `gansynth_generate` or our Pd patches!
