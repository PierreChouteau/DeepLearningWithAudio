# Training GANSynth

This guide describes how to train a GANSynth model using your own sounds. Note that training is a very resource-intensive process, so you will want to use a powerful GPU.

For training on the Triton computing cluster, see also [Training GANSynth on Triton](triton/README.md).

## Create a dataset

Prepare any amount of audio samples (the more the better) and put them in one directory. The requirements for the samples are:

- WAV format
- 16000 Hz sample rate
- 16-bit signed integer
- 4 second duration (64000 frames)
- Mono
- Named like `[instrument]_[pitch].wav`, e.g. `piano_62.wav`

GANSynth expects input in the TFRecord format (a generic file format for TensorFlow data), so the WAV files need to be converted. This can be done with our script `make_dataset.py`.

Set up a [Conda environment for GANSynth](../README.md) and activate it. Enter the `gansynth/training` directory.

Run `make_dataset.py` as follows:

```
python make_dataset.py --in_dir mysamples --out_dir mydataset
```

This will look for samples in the directory `mysamples` and produce output in the directory `mydataset`.

On success, the script outputs two files:

- `data.tfrecord` — The actual training data
- `meta.json` — Metadata about the samples

## Run the training

Have a look at [Magenta's training instructions](https://github.com/tensorflow/magenta/tree/master/magenta/models/gansynth#training) to get the basic idea.

Our version of Magenta takes some additional options in `--hparams`:

- `train_meta_path` — Path to the `meta.json` file generated by `make_dataset.py`
- `train_instrument_sources` - List of [instrument sources](https://magenta.tensorflow.org/datasets/nsynth#instrument-sources) to include (default `[0]`)
- `train_min_pitch`, `train_max_pitch` - Minimum/maximum pitches to include (default `24`, `84`)

Datasets generated by `make_datasets.py` have instrument source `0`, so it's usually not necessary to specify explicitly.

Run `gansynth_train`: 

```
gansynth_train \
    --config=mel_prog_hires \
    --hparams='{"train_data_path":"mydataset/data.tfrecord", "train_meta_path":"mydataset/meta.json", "train_root_dir":"mymodel"}'
```

Note that this is a single command, so make sure to include all the lines if copy-pasting!

This will load the training data from `mydataset/data.tfrecord` and metadata from `mydataset/meta.json`, and generate checkpoints in the `mymodel` directory.

## Postprocess the trained model

To generate sounds using the trained model, some postprocessing is required to remove intermediate files, fix up paths etc. Our `postprocess_model.py` script will do all of this. Run it as follows:

```
python postprocess_model.py --ckpt_dir mymodel --meta_path mydataset/meta.json
```

The script will list the required postprocessing steps. After checking that everything looks reasonable, re-run the script with the `--execute` flag to actually perform the postprocessing:

```
python postprocess_model.py --ckpt_dir mymodel --meta_path mydataset/meta.json --execute
```

Your model is now ready to be used with `gansynth_generate` or our Pd patches!
