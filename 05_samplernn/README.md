# SampleRNN

SampleRNN is another generative deep learning model for audio, introduced in 2017 by Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville and Yoshua Bengio. It trains on a dataset of unlabeled audio chunks and learns to generate sequences of similar audio. The architecture of SampleRNN is based on recurrent neural networks (RNNs), in which the output of each time step is fed back as input to the next one. RNNs are well suited for modeling sequential data, but suffer from the vanishing gradient problem, in which the network's ability to learn is hindered by its gradients shrinking exponentially during backpropagation. Two popular variants of RNN have been developed to address the vanishing gradient problem: gated recurrent units (GRU) and long short-term memory (LSTM). SampleRNN can be configured to work with either variant, and it's not currently clear whether either one is universally better.

The use of RNNs allows SampleRNN to generate audio sequences of any duration. This is in contrast to the fixed-length sequences of GANSynth, however SampleRNN is significantly slower. As the [original implementation of SampleRNN](https://github.com/soroushmehr/sampleRNN_ICLR2017) is unmaintained and hard to set up successfully, we use [our own fork](https://github.com/SopiMlab/prism-samplernn/) of the [PRiSM SampleRNN](https://github.com/rncm-prism/prism-samplernn/) implementation.

The character of the audio generated by SampleRNN can be influenced by a parameter known as the sampling temperature. It is also possible to use a "seed" audio as input to affect the output, but at the time of writing, this is [not working as intended](https://github.com/rncm-prism/prism-samplernn/issues/9#issuecomment-760905603).

SampleRNN has been extensively used by the group [DADABOTS](https://dadabots.com) to generate entire albums in various styles including death metal and free jazz.

## Setup (macOS/Linux)

First make sure you have [pyext](../utilities/pyext-setup/) set up.

To avoid potential version incompatibilities between SampleRNN and our existing Conda environment for Magenta, we will create a separate one for SampleRNN.

Create the environment:

----

### on macOS

```
conda create -n samplernn python=3.8
```

### on Linux

```
conda create -n samplernn python=3.8 tensorflow=2
```

If you have an NVIDIA GPU with CUDA support, you can use `tensorflow-gpu` instead for much better performance:

```
conda create -n samplernn python=3.8 tensorflow-gpu=2
```

----

Activate the environment:

```
conda activate samplernn
```

Enter the samplernn directory in the course repository (in this example, located in the home directory):

```
cd ~/DeepLearningWithAudio/05_samplernn
```

Clone our PRiSM SampleRNN repository:

```
git clone https://github.com/SopiMlab/prism-samplernn
```

Enter the resulting directory:

```
cd prism-samplernn
```

Install SampleRNN:

```
pip install -e .
```

Install the sopilib support library:

```
pip install -e ../../SopiLib/sopilib
```

Find out the path of your Python interpreter:

```
which python
```

Example output (your path may differ):

```
/usr/local/Caskroom/miniconda/base/envs/ddsp/bin/python
```

Finally, open `samplernn.pd`. You will need to edit the `load` message to use your Python executable path.

## Checkpoints

We have some pre-trained checkpoints in the [SOPI Google Drive](https://drive.google.com/drive/folders/1yoJhvr2UY0ID3AP6jumUItJJGSkiBEg_).


## SampleRNN Training in Azure My Virtual Machines

Log in to  https://labs.azure.com
(see the  [login instructions](https://github.com/SopiMlab/DeepLearningWithAudio/blob/master/00_introduction/))

c/p the command line below into your ternimnal window to go to the dlwa directory

```
cd /data/dome5132fileshare/DeepLearningWithAudio/utilities/dlwa
```



**TRANSFERING YOUR DATASET TO THE VIRTUAL MACHINE**

Please  before transferring your files and running chunk_audio, you should convert your audio to your desired sample rate (16000 Hz by default) and mono. The train script is supposed to handle this automatically, but it seems to be buggy at the moment.

You can transfer your files from your own PC to the vm following the below command line structure. Open a new terminal window make sure that you are in your own computer/laptop directory

* Transfering a folder
```
scp -P 63635 -r input_folder e5132-admin@ml-lab-00cec95c-0f8d-40ef-96bb-8837822e93b6.westeurope.cloudapp.azure.com:/data/dome5132fileshare/DeepLearningWithAudio/utilities/dlwa/inputs/your_name 
```

* Transfering a file
```
scp -P 63635 input_name.wav e5132-admin@ml-lab-00cec95c-0f8d-40ef-96bb-8837822e93b6.westeurope.cloudapp.azure.com:/data/dome5132fileshare/DeepLearningWithAudio/utilities/dlwa/inputs/your_name
```
Please note that the text **"63635"** in the command line above should be changed with your personal info. You can find it in the ssh command line in the pop up connect window. (see the  [login instructions](https://github.com/SopiMlab/DeepLearningWithAudio/blob/master/00_introduction/))

**input_folder** and should be replaced with your directory path in your own machine as well as the folder **your_name**. Please note that the name you give to **input_folder** will be used in below command lines as well.



**PREPARING YOUR DATASET**

```
./dlwa.py samplernn chunk-audio --input_name your_name/myinputs --output_name your_name/myinputs_chunks
```
**your_name/myinputs** and  **your_name/myinputs_chunks** should be replaced with your own folder names. Saves chopped files into DeepLearningWithAudio/utilities/dlwa/inputs/your_name/myinputs_chunk (It will create the folder **myinputs_chun**, don't need to create it before)



**STARTING THE TRAINING**

```
./dlwa.py samplernn train --input_name your_name/myinputs_chunks --model_name  your_name/model_name  --preset lstm-linear-skip
```
**your_name/myinputs_chunks** and  **your_name/model_name** should be replaced with your own folder names. This command line will start the SampleRNN training and it will save the trained checkpoints and logs into DeepLearningWithAudio/utilities/dlwa/models/samplernn/**your_name/model_name** and it will save the generated audio into DeepLearningWithAudio/utilities/dlwa/generated/**your_name/model_name**. Please also note that --preset lstm-linear-skip is the default choice with dlwa script.



**MONITORING THE TRAINING**

It is most likely that SampleRNN training will take approximatley 38 hours, during which you can log in and monitor the status of your training. To do that;

Log in to  https://labs.azure.com
(see the  [login instructions](https://github.com/SopiMlab/DeepLearningWithAudio/blob/master/00_introduction/))

c/p the command line below into your ternimnal window to go to the dlwa directory

```
cd /data/dome5132fileshare/DeepLearningWithAudio/utilities/dlwa
./dlwa.py util screen-attach
```

If your **traning still continues**, you will see similar output on your termninal window :

```
Epoch: 27/100, Step: 6/250, Loss: 1.355, Accuracy: 46.000, (4.365 sec/step)
Epoch: 27/100, Step: 7/250, Loss: 1.352, Accuracy: 46.179, (4.377 sec/step)
Epoch: 27/100, Step: 8/250, Loss: 1.349, Accuracy: 46.208, (4.323 sec/step)
Epoch: 27/100, Step: 9/250, Loss: 1.344, Accuracy: 46.309, (4.358 sec/step)
```

If your **traning is completed**, you will see the below text on your terminal window :

```
script failed: attach dlwa screen
aborting! 
```


**TRANSFERING YOUR TRAINED MODEL TO YOUR OWN COMPUTER/LAPTOP**

You can transfer your files, such as trained models from your the virtual machine to your on own PC  following the below command line structure. Open a new terminal window make sure that you are in your own computer/laptop directory.

* Transfering a folder

```
scp -P 63635 -r e5132-admin@ml-lab-00cec95c-0f8d-40ef-96bb-8837822e93b6.westeurope.cloudapp.azure.com:/data/dome5132fileshareDeepLearningWithAudio/utilities/dlwa/models/samplernn/your_name/model_name ~/Downloads
```

Please note that the text **"63635"** in the command line above should be changed with your personal info. You can find it in the ssh command line in the pop up connect window. (see the  [login instructions](https://github.com/SopiMlab/DeepLearningWithAudio/blob/master/00_introduction/))

**your_name/mysound** and should be replaced with your directory path in your own machine. 




## SampleRNN training in other virtual machines

See [Training SampleRNN](training.md).


## Exercises

1. Try generating some sounds with different values for the sampling temperature parameter. How does it affect the results?

## Links

- [SampleRNN: An Unconditional End-to-End Neural Audio Generation Model](https://arxiv.org/abs/1612.07837)
